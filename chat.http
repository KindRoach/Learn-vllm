### vLLM Chat Request
POST http://localhost:8000/v1/chat/completions
Content-Type: application/json

{
  "model": "/models/Qwen3-4B",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Explain how KV cache works in vLLM."}
  ],
  "temperature": 0.7,
  "max_tokens": 512
}


### vLLM Chat Request
POST http://localhost:8000/v1/chat/completions
Content-Type: application/json

{
  "model": "/models/Qwen3-4B",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What's vLLM?"}
  ],
  "temperature": 0.7,
  "max_tokens": 512
}
